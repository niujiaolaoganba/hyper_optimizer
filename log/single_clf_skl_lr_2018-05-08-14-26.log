[2018-05-08 14:26:45,250] INFO: tpe_transform took 0.001011 seconds
[2018-05-08 14:26:45,251] INFO: TPE using 0 trials
[2018-05-08 14:26:45,251] INFO: ==================================================
[2018-05-08 14:26:45,252] INFO: Task
[2018-05-08 14:26:45,252] INFO:      Learner@clf_skl_lr_Id@1
[2018-05-08 14:26:45,252] INFO: Param
[2018-05-08 14:26:45,252] INFO:      C: 2.891150767022549e-07
[2018-05-08 14:26:45,252] INFO:      penalty: l1
[2018-05-08 14:26:45,252] INFO:      random_state: 42
[2018-05-08 14:26:45,252] INFO: Result
[2018-05-08 14:26:45,252] INFO:      Run     AUC     Shape
[2018-05-08 14:26:46,044] INFO:        1          0.5     44601 x 390
[2018-05-08 14:26:46,541] INFO:        2          0.5     44601 x 390
[2018-05-08 14:26:47,069] INFO:        3          0.5     44602 x 390
[2018-05-08 14:26:47,201] INFO: AUC
[2018-05-08 14:26:47,201] INFO:      cv_mean: 0.500000
[2018-05-08 14:26:47,201] INFO:      cv_test: 0.500000
[2018-05-08 14:26:47,201] INFO: Time
[2018-05-08 14:26:47,201] INFO:      1 secs
[2018-05-08 14:26:47,202] INFO: --------------------------------------------------
[2018-05-08 14:26:47,835] INFO: tpe_transform took 0.001193 seconds
[2018-05-08 14:26:47,836] INFO: TPE using 1/1 trials with best loss -0.500000
[2018-05-08 14:26:47,837] INFO: ==================================================
[2018-05-08 14:26:47,837] INFO: Task
[2018-05-08 14:26:47,837] INFO:      Learner@clf_skl_lr_Id@2
[2018-05-08 14:26:47,837] INFO: Param
[2018-05-08 14:26:47,837] INFO:      C: 1.031187928248653
[2018-05-08 14:26:47,837] INFO:      penalty: l2
[2018-05-08 14:26:47,837] INFO:      random_state: 42
[2018-05-08 14:26:47,838] INFO: Result
[2018-05-08 14:26:47,838] INFO:      Run     AUC     Shape
[2018-05-08 14:26:49,812] INFO:        1     0.775228     44601 x 390
[2018-05-08 14:26:51,335] INFO:        2     0.794628     44601 x 390
[2018-05-08 14:26:52,889] INFO:        3     0.786915     44602 x 390
[2018-05-08 14:26:53,101] INFO: AUC
[2018-05-08 14:26:53,102] INFO:      cv_mean: 0.785590
[2018-05-08 14:26:53,102] INFO:      cv_test: 0.794178
[2018-05-08 14:26:53,102] INFO: Time
[2018-05-08 14:26:53,102] INFO:      5 secs
[2018-05-08 14:26:53,102] INFO: --------------------------------------------------
[2018-05-08 14:26:55,878] INFO: tpe_transform took 0.001618 seconds
[2018-05-08 14:26:55,878] INFO: TPE using 2/2 trials with best loss -0.793983
[2018-05-08 14:26:55,879] INFO: ==================================================
[2018-05-08 14:26:55,879] INFO: Task
[2018-05-08 14:26:55,879] INFO:      Learner@clf_skl_lr_Id@3
[2018-05-08 14:26:55,880] INFO: Param
[2018-05-08 14:26:55,880] INFO:      C: 2.7343172930869926e-06
[2018-05-08 14:26:55,880] INFO:      penalty: l1
[2018-05-08 14:26:55,880] INFO:      random_state: 42
[2018-05-08 14:26:55,880] INFO: Result
[2018-05-08 14:26:55,880] INFO:      Run     AUC     Shape
[2018-05-08 14:26:56,387] INFO:        1          0.5     44601 x 390
[2018-05-08 14:26:56,943] INFO:        2          0.5     44601 x 390
[2018-05-08 14:26:57,467] INFO:        3          0.5     44602 x 390
[2018-05-08 14:26:57,592] INFO: AUC
[2018-05-08 14:26:57,592] INFO:      cv_mean: 0.500000
[2018-05-08 14:26:57,592] INFO:      cv_test: 0.500000
[2018-05-08 14:26:57,592] INFO: Time
[2018-05-08 14:26:57,592] INFO:      1 secs
[2018-05-08 14:26:57,593] INFO: --------------------------------------------------
[2018-05-08 14:26:58,234] INFO: tpe_transform took 0.003439 seconds
[2018-05-08 14:26:58,235] INFO: TPE using 3/3 trials with best loss -0.793983
[2018-05-08 14:26:58,236] INFO: ==================================================
[2018-05-08 14:26:58,236] INFO: Task
[2018-05-08 14:26:58,236] INFO:      Learner@clf_skl_lr_Id@4
[2018-05-08 14:26:58,236] INFO: Param
[2018-05-08 14:26:58,236] INFO:      C: 0.0022748043833056997
[2018-05-08 14:26:58,236] INFO:      penalty: l1
[2018-05-08 14:26:58,236] INFO:      random_state: 42
[2018-05-08 14:26:58,236] INFO: Result
[2018-05-08 14:26:58,236] INFO:      Run     AUC     Shape
[2018-05-08 14:26:59,309] INFO:        1     0.735245     44601 x 390
[2018-05-08 14:27:00,315] INFO:        2     0.750799     44601 x 390
[2018-05-08 14:27:01,436] INFO:        3     0.744364     44602 x 390
[2018-05-08 14:27:01,647] INFO: AUC
[2018-05-08 14:27:01,647] INFO:      cv_mean: 0.743470
[2018-05-08 14:27:01,647] INFO:      cv_test: 0.749457
[2018-05-08 14:27:01,647] INFO: Time
[2018-05-08 14:27:01,647] INFO:      3 secs
[2018-05-08 14:27:01,647] INFO: --------------------------------------------------
[2018-05-08 14:27:03,135] INFO: tpe_transform took 0.000893 seconds
[2018-05-08 14:27:03,135] INFO: TPE using 4/4 trials with best loss -0.793983
[2018-05-08 14:27:03,136] INFO: ==================================================
[2018-05-08 14:27:03,136] INFO: Task
[2018-05-08 14:27:03,136] INFO:      Learner@clf_skl_lr_Id@5
[2018-05-08 14:27:03,136] INFO: Param
[2018-05-08 14:27:03,136] INFO:      C: 10.528489855201213
[2018-05-08 14:27:03,136] INFO:      penalty: l2
[2018-05-08 14:27:03,136] INFO:      random_state: 42
[2018-05-08 14:27:03,136] INFO: Result
[2018-05-08 14:27:03,136] INFO:      Run     AUC     Shape
[2018-05-08 14:27:06,307] INFO:        1     0.771807     44601 x 390
[2018-05-08 14:27:08,901] INFO:        2     0.793327     44601 x 390
[2018-05-08 14:27:11,982] INFO:        3     0.785704     44602 x 390
[2018-05-08 14:27:12,202] INFO: AUC
[2018-05-08 14:27:12,202] INFO:      cv_mean: 0.783613
[2018-05-08 14:27:12,202] INFO:      cv_test: 0.793092
[2018-05-08 14:27:12,203] INFO: Time
[2018-05-08 14:27:12,203] INFO:      8 secs
[2018-05-08 14:27:12,203] INFO: --------------------------------------------------
[2018-05-08 14:27:17,285] INFO: tpe_transform took 0.000991 seconds
[2018-05-08 14:27:17,285] INFO: TPE using 5/5 trials with best loss -0.793983
[2018-05-08 14:27:17,288] INFO: ==================================================
[2018-05-08 14:27:17,288] INFO: Task
[2018-05-08 14:27:17,288] INFO:      Learner@clf_skl_lr_Id@6
[2018-05-08 14:27:17,288] INFO: Param
[2018-05-08 14:27:17,288] INFO:      C: 2.0780754683865378e-07
[2018-05-08 14:27:17,288] INFO:      penalty: l2
[2018-05-08 14:27:17,288] INFO:      random_state: 42
[2018-05-08 14:27:17,288] INFO: Result
[2018-05-08 14:27:17,289] INFO:      Run     AUC     Shape
[2018-05-08 14:27:17,826] INFO:        1     0.348069     44601 x 390
[2018-05-08 14:27:18,311] INFO:        2     0.349215     44601 x 390
[2018-05-08 14:27:18,790] INFO:        3     0.354847     44602 x 390
[2018-05-08 14:27:19,002] INFO: AUC
[2018-05-08 14:27:19,003] INFO:      cv_mean: 0.350710
[2018-05-08 14:27:19,003] INFO:      cv_test: 0.343370
[2018-05-08 14:27:19,003] INFO: Time
[2018-05-08 14:27:19,003] INFO:      1 secs
[2018-05-08 14:27:19,003] INFO: --------------------------------------------------
[2018-05-08 14:27:19,733] INFO: tpe_transform took 0.000969 seconds
[2018-05-08 14:27:19,733] INFO: TPE using 6/6 trials with best loss -0.793983
[2018-05-08 14:27:19,734] INFO: ==================================================
[2018-05-08 14:27:19,734] INFO: Task
[2018-05-08 14:27:19,734] INFO:      Learner@clf_skl_lr_Id@7
[2018-05-08 14:27:19,735] INFO: Param
[2018-05-08 14:27:19,735] INFO:      C: 0.42887267446805283
[2018-05-08 14:27:19,735] INFO:      penalty: l1
[2018-05-08 14:27:19,735] INFO:      random_state: 42
[2018-05-08 14:27:19,735] INFO: Result
[2018-05-08 14:27:19,735] INFO:      Run     AUC     Shape
[2018-05-08 14:27:28,130] INFO:        1     0.779002     44601 x 390
[2018-05-08 14:27:35,577] INFO:        2     0.797928     44601 x 390
[2018-05-08 14:27:47,792] INFO:        3     0.789766     44602 x 390
[2018-05-08 14:27:48,008] INFO: AUC
[2018-05-08 14:27:48,009] INFO:      cv_mean: 0.788898
[2018-05-08 14:27:48,009] INFO:      cv_test: 0.795536
[2018-05-08 14:27:48,009] INFO: Time
[2018-05-08 14:27:48,009] INFO:      28 secs
[2018-05-08 14:27:48,009] INFO: --------------------------------------------------
[2018-05-08 14:28:02,189] INFO: tpe_transform took 0.000893 seconds
[2018-05-08 14:28:02,189] INFO: TPE using 7/7 trials with best loss -0.795195
[2018-05-08 14:28:02,192] INFO: ==================================================
[2018-05-08 14:28:02,192] INFO: Task
[2018-05-08 14:28:02,192] INFO:      Learner@clf_skl_lr_Id@8
[2018-05-08 14:28:02,192] INFO: Param
[2018-05-08 14:28:02,192] INFO:      C: 1.3189442285436657e-05
[2018-05-08 14:28:02,193] INFO:      penalty: l1
[2018-05-08 14:28:02,193] INFO:      random_state: 42
[2018-05-08 14:28:02,193] INFO: Result
[2018-05-08 14:28:02,193] INFO:      Run     AUC     Shape
[2018-05-08 14:28:02,702] INFO:        1          0.5     44601 x 390
[2018-05-08 14:28:03,226] INFO:        2          0.5     44601 x 390
[2018-05-08 14:28:03,723] INFO:        3          0.5     44602 x 390
[2018-05-08 14:28:03,845] INFO: AUC
[2018-05-08 14:28:03,845] INFO:      cv_mean: 0.500000
[2018-05-08 14:28:03,845] INFO:      cv_test: 0.500000
[2018-05-08 14:28:03,845] INFO: Time
[2018-05-08 14:28:03,845] INFO:      1 secs
[2018-05-08 14:28:03,845] INFO: --------------------------------------------------
[2018-05-08 14:28:04,489] INFO: tpe_transform took 0.000881 seconds
[2018-05-08 14:28:04,489] INFO: TPE using 8/8 trials with best loss -0.795195
[2018-05-08 14:28:04,490] INFO: ==================================================
[2018-05-08 14:28:04,490] INFO: Task
[2018-05-08 14:28:04,491] INFO:      Learner@clf_skl_lr_Id@9
[2018-05-08 14:28:04,491] INFO: Param
[2018-05-08 14:28:04,491] INFO:      C: 3.1394818232538376e-05
[2018-05-08 14:28:04,491] INFO:      penalty: l2
[2018-05-08 14:28:04,491] INFO:      random_state: 42
[2018-05-08 14:28:04,491] INFO: Result
[2018-05-08 14:28:04,491] INFO:      Run     AUC     Shape
[2018-05-08 14:28:05,037] INFO:        1     0.428997     44601 x 390
[2018-05-08 14:28:05,541] INFO:        2     0.430879     44601 x 390
[2018-05-08 14:28:06,063] INFO:        3     0.435122     44602 x 390
[2018-05-08 14:28:06,282] INFO: AUC
[2018-05-08 14:28:06,282] INFO:      cv_mean: 0.431666
[2018-05-08 14:28:06,282] INFO:      cv_test: 0.428869
[2018-05-08 14:28:06,282] INFO: Time
[2018-05-08 14:28:06,282] INFO:      1 secs
[2018-05-08 14:28:06,283] INFO: --------------------------------------------------
[2018-05-08 14:28:07,089] INFO: Hyperopt_Time
[2018-05-08 14:28:07,089] INFO:      1 mins
[2018-05-08 14:28:07,089] INFO: --------------------------------------------------
[2018-05-08 14:28:07,108] INFO: tpe_transform took 0.004555 seconds
[2018-05-08 14:28:07,108] INFO: TPE using 0 trials
[2018-05-08 14:28:07,111] INFO: ==================================================
[2018-05-08 14:28:07,111] INFO: Task
[2018-05-08 14:28:07,111] INFO:      Learner@clf_xgb_tree_Id@10
[2018-05-08 14:28:07,111] INFO: Param
[2018-05-08 14:28:07,111] INFO:      colsample_bylevel: 0.8234472932335852
[2018-05-08 14:28:07,111] INFO:      colsample_bytree: 0.8256875806633304
[2018-05-08 14:28:07,112] INFO:      gamma: 0.002444296584700245
[2018-05-08 14:28:07,112] INFO:      learning_rate: 0.012
[2018-05-08 14:28:07,112] INFO:      max_depth: 8
[2018-05-08 14:28:07,112] INFO:      min_child_weight: 0.0002316818898171937
[2018-05-08 14:28:07,112] INFO:      n_estimators: 400
[2018-05-08 14:28:07,112] INFO:      nthread: 8
[2018-05-08 14:28:07,113] INFO:      reg_alpha: 0.021879433762834792
[2018-05-08 14:28:07,113] INFO:      reg_lambda: 2.2496701264304494e-10
[2018-05-08 14:28:07,113] INFO:      seed: 42
[2018-05-08 14:28:07,113] INFO:      subsample: 0.7155196955240777
[2018-05-08 14:28:07,113] INFO: Result
[2018-05-08 14:28:07,113] INFO:      Run     AUC     Shape
[2018-05-08 14:32:52,413] INFO:        1     0.785236     44601 x 390
[2018-05-08 14:37:34,566] INFO:        2     0.799616     44601 x 390
[2018-05-08 14:42:18,991] INFO:        3     0.790101     44602 x 390
[2018-05-08 14:42:19,224] INFO: AUC
[2018-05-08 14:42:19,224] INFO:      cv_mean: 0.791651
[2018-05-08 14:42:19,224] INFO:      cv_test: 0.797578
[2018-05-08 14:42:19,225] INFO: Time
[2018-05-08 14:42:19,225] INFO:      14 mins
[2018-05-08 14:42:19,225] INFO: --------------------------------------------------
[2018-05-08 14:49:27,576] INFO: tpe_transform took 0.005043 seconds
[2018-05-08 14:49:27,577] INFO: TPE using 1/1 trials with best loss -0.797512
[2018-05-08 14:49:27,579] INFO: ==================================================
[2018-05-08 14:49:27,579] INFO: Task
[2018-05-08 14:49:27,579] INFO:      Learner@clf_xgb_tree_Id@11
[2018-05-08 14:49:27,579] INFO: Param
[2018-05-08 14:49:27,579] INFO:      colsample_bylevel: 0.9633488334814934
[2018-05-08 14:49:27,579] INFO:      colsample_bytree: 0.7183197629124758
[2018-05-08 14:49:27,580] INFO:      gamma: 2.7412925652565203e-10
[2018-05-08 14:49:27,580] INFO:      learning_rate: 0.01
[2018-05-08 14:49:27,580] INFO:      max_depth: 3
[2018-05-08 14:49:27,580] INFO:      min_child_weight: 0.2759552783464698
[2018-05-08 14:49:27,580] INFO:      n_estimators: 930
[2018-05-08 14:49:27,580] INFO:      nthread: 8
[2018-05-08 14:49:27,580] INFO:      reg_alpha: 0.0006903932233771911
[2018-05-08 14:49:27,580] INFO:      reg_lambda: 0.0008640806558159575
[2018-05-08 14:49:27,581] INFO:      seed: 42
[2018-05-08 14:49:27,581] INFO:      subsample: 0.991300588693812
[2018-05-08 14:49:27,581] INFO: Result
[2018-05-08 14:49:27,581] INFO:      Run     AUC     Shape
[2018-05-08 14:52:59,091] INFO:        1     0.781919     44601 x 390
[2018-05-08 14:56:31,524] INFO:        2     0.798584     44601 x 390
[2018-05-08 15:00:06,187] INFO:        3     0.789644     44602 x 390
[2018-05-08 15:00:06,416] INFO: AUC
[2018-05-08 15:00:06,416] INFO:      cv_mean: 0.790049
[2018-05-08 15:00:06,416] INFO:      cv_test: 0.793858
[2018-05-08 15:00:06,417] INFO: Time
[2018-05-08 15:00:06,417] INFO:      10 mins
[2018-05-08 15:00:06,417] INFO: --------------------------------------------------
[2018-05-08 15:05:30,121] INFO: tpe_transform took 0.005552 seconds
[2018-05-08 15:05:30,122] INFO: TPE using 2/2 trials with best loss -0.797512
[2018-05-08 15:05:30,124] INFO: ==================================================
[2018-05-08 15:05:30,124] INFO: Task
[2018-05-08 15:05:30,125] INFO:      Learner@clf_xgb_tree_Id@12
[2018-05-08 15:05:30,125] INFO: Param
[2018-05-08 15:05:30,125] INFO:      colsample_bylevel: 0.5725961745918822
[2018-05-08 15:05:30,125] INFO:      colsample_bytree: 0.730918257044071
[2018-05-08 15:05:30,125] INFO:      gamma: 1.6396002943419893e-06
[2018-05-08 15:05:30,125] INFO:      learning_rate: 0.02
[2018-05-08 15:05:30,125] INFO:      max_depth: 8
[2018-05-08 15:05:30,125] INFO:      min_child_weight: 32.19887927894507
[2018-05-08 15:05:30,125] INFO:      n_estimators: 600
[2018-05-08 15:05:30,126] INFO:      nthread: 8
[2018-05-08 15:05:30,126] INFO:      reg_alpha: 8.135434199207248e-06
[2018-05-08 15:05:30,126] INFO:      reg_lambda: 8.433860734016e-05
[2018-05-08 15:05:30,126] INFO:      seed: 42
[2018-05-08 15:05:30,126] INFO:      subsample: 0.9834838985125951
[2018-05-08 15:05:30,126] INFO: Result
[2018-05-08 15:05:30,126] INFO:      Run     AUC     Shape
[2018-05-08 15:09:20,443] INFO:        1     0.785313     44601 x 390
[2018-05-08 15:13:05,572] INFO:        2     0.805003     44601 x 390
[2018-05-08 15:16:50,995] INFO:        3     0.794495     44602 x 390
[2018-05-08 15:16:51,189] INFO: AUC
[2018-05-08 15:16:51,190] INFO:      cv_mean: 0.794937
[2018-05-08 15:16:51,190] INFO:      cv_test: 0.799292
[2018-05-08 15:16:51,190] INFO: Time
[2018-05-08 15:16:51,190] INFO:      11 mins
[2018-05-08 15:16:51,190] INFO: --------------------------------------------------
[2018-05-08 15:22:29,415] INFO: tpe_transform took 0.005869 seconds
[2018-05-08 15:22:29,415] INFO: TPE using 3/3 trials with best loss -0.798552
[2018-05-08 15:22:29,417] INFO: ==================================================
[2018-05-08 15:22:29,418] INFO: Task
[2018-05-08 15:22:29,418] INFO:      Learner@clf_xgb_tree_Id@13
[2018-05-08 15:22:29,418] INFO: Param
[2018-05-08 15:22:29,418] INFO:      colsample_bylevel: 0.5214712141768119
[2018-05-08 15:22:29,418] INFO:      colsample_bytree: 0.803778990805655
[2018-05-08 15:22:29,418] INFO:      gamma: 5.101332964638279e-06
[2018-05-08 15:22:29,418] INFO:      learning_rate: 0.024
[2018-05-08 15:22:29,418] INFO:      max_depth: 3
[2018-05-08 15:22:29,418] INFO:      min_child_weight: 0.15642852319236317
[2018-05-08 15:22:29,419] INFO:      n_estimators: 960
[2018-05-08 15:22:29,419] INFO:      nthread: 8
[2018-05-08 15:22:29,419] INFO:      reg_alpha: 2.406771556260825e-07
[2018-05-08 15:22:29,419] INFO:      reg_lambda: 6.993899022482989e-09
[2018-05-08 15:22:29,419] INFO:      seed: 42
[2018-05-08 15:22:29,419] INFO:      subsample: 0.8058602737786018
[2018-05-08 15:22:29,419] INFO: Result
[2018-05-08 15:22:29,419] INFO:      Run     AUC     Shape
[2018-05-08 15:25:14,209] INFO:        1     0.784526     44601 x 390
[2018-05-08 15:27:59,290] INFO:        2     0.802558     44601 x 390
[2018-05-08 15:30:45,546] INFO:        3     0.794158     44602 x 390
[2018-05-08 15:30:45,784] INFO: AUC
[2018-05-08 15:30:45,784] INFO:      cv_mean: 0.793747
[2018-05-08 15:30:45,784] INFO:      cv_test: 0.798159
[2018-05-08 15:30:45,784] INFO: Time
[2018-05-08 15:30:45,784] INFO:      8 mins
[2018-05-08 15:30:45,784] INFO: --------------------------------------------------
[2018-05-08 15:34:54,693] INFO: tpe_transform took 0.005672 seconds
[2018-05-08 15:34:54,693] INFO: TPE using 4/4 trials with best loss -0.798552
[2018-05-08 15:34:54,696] INFO: ==================================================
[2018-05-08 15:34:54,696] INFO: Task
[2018-05-08 15:34:54,696] INFO:      Learner@clf_xgb_tree_Id@14
[2018-05-08 15:34:54,696] INFO: Param
[2018-05-08 15:34:54,696] INFO:      colsample_bylevel: 0.6690466603694637
[2018-05-08 15:34:54,696] INFO:      colsample_bytree: 0.66598417367951
[2018-05-08 15:34:54,697] INFO:      gamma: 0.045331256849680555
[2018-05-08 15:34:54,697] INFO:      learning_rate: 0.002
[2018-05-08 15:34:54,697] INFO:      max_depth: 10
[2018-05-08 15:34:54,697] INFO:      min_child_weight: 1.107913211631579e-06
[2018-05-08 15:34:54,697] INFO:      n_estimators: 820
[2018-05-08 15:34:54,697] INFO:      nthread: 8
[2018-05-08 15:34:54,697] INFO:      reg_alpha: 0.00041928637637088633
[2018-05-08 15:34:54,697] INFO:      reg_lambda: 0.0002341711847033256
[2018-05-08 15:34:54,698] INFO:      seed: 42
[2018-05-08 15:34:54,698] INFO:      subsample: 0.8183236767819044
[2018-05-08 15:34:54,698] INFO: Result
[2018-05-08 15:34:54,698] INFO:      Run     AUC     Shape
[2018-05-08 15:42:54,215] INFO:        1     0.780832     44601 x 390
[2018-05-08 15:50:53,337] INFO:        2     0.795025     44601 x 390
[2018-05-08 15:58:36,412] INFO:        3     0.785381     44602 x 390
[2018-05-08 15:58:36,636] INFO: AUC
[2018-05-08 15:58:36,636] INFO:      cv_mean: 0.787079
[2018-05-08 15:58:36,636] INFO:      cv_test: 0.793493
[2018-05-08 15:58:36,637] INFO: Time
[2018-05-08 15:58:36,637] INFO:      23 mins
[2018-05-08 15:58:36,637] INFO: --------------------------------------------------
[2018-05-08 16:10:48,982] INFO: tpe_transform took 0.004900 seconds
[2018-05-08 16:10:48,983] INFO: TPE using 5/5 trials with best loss -0.798552
[2018-05-08 16:10:48,991] INFO: ==================================================
[2018-05-08 16:10:48,991] INFO: Task
[2018-05-08 16:10:48,991] INFO:      Learner@clf_xgb_tree_Id@15
[2018-05-08 16:10:48,992] INFO: Param
[2018-05-08 16:10:48,992] INFO:      colsample_bylevel: 0.6172534383465536
[2018-05-08 16:10:48,992] INFO:      colsample_bytree: 0.881375929891597
[2018-05-08 16:10:48,992] INFO:      gamma: 3.569208139704969e-08
[2018-05-08 16:10:48,992] INFO:      learning_rate: 0.028
[2018-05-08 16:10:48,992] INFO:      max_depth: 3
[2018-05-08 16:10:48,992] INFO:      min_child_weight: 3.566352750275266e-06
[2018-05-08 16:10:48,993] INFO:      n_estimators: 180
[2018-05-08 16:10:48,993] INFO:      nthread: 8
[2018-05-08 16:10:48,993] INFO:      reg_alpha: 0.009891521095457895
[2018-05-08 16:10:48,993] INFO:      reg_lambda: 2.3151816669778576e-08
[2018-05-08 16:10:48,993] INFO:      seed: 42
[2018-05-08 16:10:48,993] INFO:      subsample: 0.9943122905725376
[2018-05-08 16:10:48,993] INFO: Result
[2018-05-08 16:10:48,993] INFO:      Run     AUC     Shape
[2018-05-08 16:11:23,380] INFO:        1     0.777239     44601 x 390
[2018-05-08 16:11:58,209] INFO:        2     0.793492     44601 x 390
[2018-05-08 16:12:33,246] INFO:        3     0.783599     44602 x 390
[2018-05-08 16:12:33,465] INFO: AUC
[2018-05-08 16:12:33,465] INFO:      cv_mean: 0.784777
[2018-05-08 16:12:33,466] INFO:      cv_test: 0.787723
[2018-05-08 16:12:33,466] INFO: Time
[2018-05-08 16:12:33,466] INFO:      1 mins
[2018-05-08 16:12:33,466] INFO: --------------------------------------------------
[2018-05-08 16:13:24,645] INFO: tpe_transform took 0.005234 seconds
[2018-05-08 16:13:24,645] INFO: TPE using 6/6 trials with best loss -0.798552
[2018-05-08 16:13:24,648] INFO: ==================================================
[2018-05-08 16:13:24,648] INFO: Task
[2018-05-08 16:13:24,648] INFO:      Learner@clf_xgb_tree_Id@16
[2018-05-08 16:13:24,648] INFO: Param
[2018-05-08 16:13:24,648] INFO:      colsample_bylevel: 0.6646669042004119
[2018-05-08 16:13:24,648] INFO:      colsample_bytree: 0.8008239586649852
[2018-05-08 16:13:24,648] INFO:      gamma: 0.0015189341471976586
[2018-05-08 16:13:24,648] INFO:      learning_rate: 0.03
[2018-05-08 16:13:24,648] INFO:      max_depth: 8
[2018-05-08 16:13:24,649] INFO:      min_child_weight: 3.487293253615493e-07
[2018-05-08 16:13:24,649] INFO:      n_estimators: 380
[2018-05-08 16:13:24,649] INFO:      nthread: 8
[2018-05-08 16:13:24,649] INFO:      reg_alpha: 9.174279781272688e-07
[2018-05-08 16:13:24,649] INFO:      reg_lambda: 7.865805487578934
[2018-05-08 16:13:24,649] INFO:      seed: 42
[2018-05-08 16:13:24,649] INFO:      subsample: 0.7328079157923393
[2018-05-08 16:13:24,649] INFO: Result
[2018-05-08 16:13:24,649] INFO:      Run     AUC     Shape
[2018-05-08 16:16:58,854] INFO:        1     0.785009     44601 x 390
[2018-05-08 16:20:33,578] INFO:        2     0.804342     44601 x 390
[2018-05-08 16:24:07,605] INFO:        3     0.793991     44602 x 390
[2018-05-08 16:24:07,839] INFO: AUC
[2018-05-08 16:24:07,839] INFO:      cv_mean: 0.794447
[2018-05-08 16:24:07,840] INFO:      cv_test: 0.799945
[2018-05-08 16:24:07,840] INFO: Time
[2018-05-08 16:24:07,840] INFO:      10 mins
[2018-05-08 16:24:07,840] INFO: --------------------------------------------------
[2018-05-08 16:29:32,428] INFO: tpe_transform took 0.006192 seconds
[2018-05-08 16:29:32,440] INFO: TPE using 7/7 trials with best loss -0.799047
[2018-05-08 16:29:32,443] INFO: ==================================================
[2018-05-08 16:29:32,444] INFO: Task
[2018-05-08 16:29:32,444] INFO:      Learner@clf_xgb_tree_Id@17
[2018-05-08 16:29:32,444] INFO: Param
[2018-05-08 16:29:32,444] INFO:      colsample_bylevel: 0.5042415372530713
[2018-05-08 16:29:32,444] INFO:      colsample_bytree: 0.6445801988892873
[2018-05-08 16:29:32,445] INFO:      gamma: 0.45454958609294094
[2018-05-08 16:29:32,445] INFO:      learning_rate: 0.006
[2018-05-08 16:29:32,445] INFO:      max_depth: 3
[2018-05-08 16:29:32,445] INFO:      min_child_weight: 0.00014977300602245596
[2018-05-08 16:29:32,445] INFO:      n_estimators: 610
[2018-05-08 16:29:32,445] INFO:      nthread: 8
[2018-05-08 16:29:32,446] INFO:      reg_alpha: 0.0003147123170026589
[2018-05-08 16:29:32,446] INFO:      reg_lambda: 0.48514475947368174
[2018-05-08 16:29:32,446] INFO:      seed: 42
[2018-05-08 16:29:32,446] INFO:      subsample: 0.9173176905761946
[2018-05-08 16:29:32,446] INFO: Result
[2018-05-08 16:29:32,446] INFO:      Run     AUC     Shape
[2018-05-08 16:30:53,905] INFO:        1     0.772441     44601 x 390
[2018-05-08 16:32:05,539] INFO:        2      0.78965     44601 x 390
[2018-05-08 16:33:24,985] INFO:        3     0.779653     44602 x 390
[2018-05-08 16:33:25,208] INFO: AUC
[2018-05-08 16:33:25,208] INFO:      cv_mean: 0.780581
[2018-05-08 16:33:25,208] INFO:      cv_test: 0.785639
[2018-05-08 16:33:25,208] INFO: Time
[2018-05-08 16:33:25,208] INFO:      3 mins
[2018-05-08 16:33:25,208] INFO: --------------------------------------------------
[2018-05-08 16:35:26,790] INFO: tpe_transform took 0.004736 seconds
[2018-05-08 16:35:26,791] INFO: TPE using 8/8 trials with best loss -0.799047
[2018-05-08 16:35:26,793] INFO: ==================================================
[2018-05-08 16:35:26,793] INFO: Task
[2018-05-08 16:35:26,793] INFO:      Learner@clf_xgb_tree_Id@18
[2018-05-08 16:35:26,793] INFO: Param
[2018-05-08 16:35:26,794] INFO:      colsample_bylevel: 0.8228649096189459
[2018-05-08 16:35:26,794] INFO:      colsample_bytree: 0.7394020788661573
[2018-05-08 16:35:26,794] INFO:      gamma: 0.0019201213676120301
[2018-05-08 16:35:26,794] INFO:      learning_rate: 0.032
[2018-05-08 16:35:26,794] INFO:      max_depth: 8
[2018-05-08 16:35:26,794] INFO:      min_child_weight: 0.0011862350085405373
[2018-05-08 16:35:26,794] INFO:      n_estimators: 370
[2018-05-08 16:35:26,794] INFO:      nthread: 8
[2018-05-08 16:35:26,794] INFO:      reg_alpha: 0.03248741799950571
[2018-05-08 16:35:26,795] INFO:      reg_lambda: 2.3895526007289842e-09
[2018-05-08 16:35:26,795] INFO:      seed: 42
[2018-05-08 16:35:26,795] INFO:      subsample: 0.850353966919666
[2018-05-08 16:35:26,795] INFO: Result
[2018-05-08 16:35:26,795] INFO:      Run     AUC     Shape
[2018-05-08 16:38:56,311] INFO:        1      0.77956     44601 x 390
[2018-05-08 16:42:20,808] INFO:        2     0.800192     44601 x 390
[2018-05-08 16:45:32,094] INFO:        3     0.790298     44602 x 390
[2018-05-08 16:45:32,301] INFO: AUC
[2018-05-08 16:45:32,301] INFO:      cv_mean: 0.790017
[2018-05-08 16:45:32,301] INFO:      cv_test: 0.797722
[2018-05-08 16:45:32,302] INFO: Time
[2018-05-08 16:45:32,302] INFO:      10 mins
[2018-05-08 16:45:32,302] INFO: --------------------------------------------------
[2018-05-08 16:50:19,812] INFO: Hyperopt_Time
[2018-05-08 16:50:19,813] INFO:      142 mins
[2018-05-08 16:50:19,813] INFO: --------------------------------------------------
[2018-05-08 16:50:19,826] INFO: tpe_transform took 0.002001 seconds
[2018-05-08 16:50:19,827] INFO: TPE using 0 trials
[2018-05-08 16:50:19,828] INFO: ==================================================
[2018-05-08 16:50:19,829] INFO: Task
[2018-05-08 16:50:19,829] INFO:      Learner@clf_skl_rf_Id@19
[2018-05-08 16:50:19,829] INFO: Param
[2018-05-08 16:50:19,829] INFO:      max_depth: 2
[2018-05-08 16:50:19,829] INFO:      max_features: 0.8500000000000001
[2018-05-08 16:50:19,829] INFO:      min_samples_leaf: 13
[2018-05-08 16:50:19,830] INFO:      min_samples_split: 6
[2018-05-08 16:50:19,830] INFO:      n_estimators: 360
[2018-05-08 16:50:19,830] INFO:      n_jobs: 8
[2018-05-08 16:50:19,830] INFO:      random_state: 42
[2018-05-08 16:50:19,830] INFO:      verbose: 0
[2018-05-08 16:50:19,831] INFO: Result
[2018-05-08 16:50:19,831] INFO:      Run     AUC     Shape
[2018-05-08 16:50:47,429] INFO:        1     0.723137     44601 x 390
[2018-05-08 16:51:14,516] INFO:        2     0.750775     44601 x 390
[2018-05-08 16:51:43,241] INFO:        3     0.757087     44602 x 390
[2018-05-08 16:51:43,436] INFO: AUC
[2018-05-08 16:51:43,436] INFO:      cv_mean: 0.743666
[2018-05-08 16:51:43,436] INFO:      cv_test: 0.762075
[2018-05-08 16:51:43,436] INFO: Time
[2018-05-08 16:51:43,436] INFO:      1 mins
[2018-05-08 16:51:43,436] INFO: --------------------------------------------------
[2018-05-08 16:52:33,720] INFO: tpe_transform took 0.004635 seconds
[2018-05-08 16:52:33,720] INFO: TPE using 1/1 trials with best loss -0.759192
[2018-05-08 16:52:33,722] INFO: ==================================================
[2018-05-08 16:52:33,722] INFO: Task
[2018-05-08 16:52:33,722] INFO:      Learner@clf_skl_rf_Id@20
[2018-05-08 16:52:33,723] INFO: Param
[2018-05-08 16:52:33,723] INFO:      max_depth: 7
[2018-05-08 16:52:33,723] INFO:      max_features: 0.4
[2018-05-08 16:52:33,723] INFO:      min_samples_leaf: 8
[2018-05-08 16:52:33,724] INFO:      min_samples_split: 9
[2018-05-08 16:52:33,724] INFO:      n_estimators: 400
[2018-05-08 16:52:33,724] INFO:      n_jobs: 8
[2018-05-08 16:52:33,724] INFO:      random_state: 42
[2018-05-08 16:52:33,724] INFO:      verbose: 0
[2018-05-08 16:52:33,725] INFO: Result
[2018-05-08 16:52:33,725] INFO:      Run     AUC     Shape
[2018-05-08 16:53:26,681] INFO:        1     0.774531     44601 x 390
[2018-05-08 16:54:15,916] INFO:        2     0.790219     44601 x 390
[2018-05-08 16:55:03,497] INFO:        3     0.780534     44602 x 390
[2018-05-08 16:55:03,711] INFO: AUC
[2018-05-08 16:55:03,711] INFO:      cv_mean: 0.781761
[2018-05-08 16:55:03,712] INFO:      cv_test: 0.788283
[2018-05-08 16:55:03,712] INFO: Time
[2018-05-08 16:55:03,712] INFO:      2 mins
[2018-05-08 16:55:03,712] INFO: --------------------------------------------------
[2018-05-08 16:56:27,558] INFO: tpe_transform took 0.002241 seconds
[2018-05-08 16:56:27,558] INFO: TPE using 2/2 trials with best loss -0.787796
[2018-05-08 16:56:27,560] INFO: ==================================================
[2018-05-08 16:56:27,560] INFO: Task
[2018-05-08 16:56:27,560] INFO:      Learner@clf_skl_rf_Id@21
[2018-05-08 16:56:27,560] INFO: Param
[2018-05-08 16:56:27,560] INFO:      max_depth: 7
[2018-05-08 16:56:27,561] INFO:      max_features: 0.4
[2018-05-08 16:56:27,561] INFO:      min_samples_leaf: 15
[2018-05-08 16:56:27,561] INFO:      min_samples_split: 7
[2018-05-08 16:56:27,561] INFO:      n_estimators: 100
[2018-05-08 16:56:27,561] INFO:      n_jobs: 8
[2018-05-08 16:56:27,561] INFO:      random_state: 42
[2018-05-08 16:56:27,561] INFO:      verbose: 0
[2018-05-08 16:56:27,562] INFO: Result
[2018-05-08 16:56:27,562] INFO:      Run     AUC     Shape
[2018-05-08 16:56:39,413] INFO:        1     0.774177     44601 x 390
[2018-05-08 16:56:52,188] INFO:        2     0.790508     44601 x 390
[2018-05-08 16:57:04,459] INFO:        3     0.780039     44602 x 390
[2018-05-08 16:57:04,659] INFO: AUC
[2018-05-08 16:57:04,660] INFO:      cv_mean: 0.781574
[2018-05-08 16:57:04,660] INFO:      cv_test: 0.788834
[2018-05-08 16:57:04,660] INFO: Time
[2018-05-08 16:57:04,661] INFO:      36 secs
[2018-05-08 16:57:04,661] INFO: --------------------------------------------------
[2018-05-08 16:57:26,216] INFO: tpe_transform took 0.001880 seconds
[2018-05-08 16:57:26,216] INFO: TPE using 3/3 trials with best loss -0.788916
[2018-05-08 16:57:26,218] INFO: ==================================================
[2018-05-08 16:57:26,218] INFO: Task
[2018-05-08 16:57:26,218] INFO:      Learner@clf_skl_rf_Id@22
[2018-05-08 16:57:26,218] INFO: Param
[2018-05-08 16:57:26,218] INFO:      max_depth: 6
[2018-05-08 16:57:26,218] INFO:      max_features: 0.35000000000000003
[2018-05-08 16:57:26,219] INFO:      min_samples_leaf: 5
[2018-05-08 16:57:26,219] INFO:      min_samples_split: 8
[2018-05-08 16:57:26,219] INFO:      n_estimators: 410
[2018-05-08 16:57:26,219] INFO:      n_jobs: 8
[2018-05-08 16:57:26,219] INFO:      random_state: 42
[2018-05-08 16:57:26,219] INFO:      verbose: 0
[2018-05-08 16:57:26,219] INFO: Result
[2018-05-08 16:57:26,220] INFO:      Run     AUC     Shape
[2018-05-08 16:58:03,114] INFO:        1     0.771316     44601 x 390
[2018-05-08 16:58:40,703] INFO:        2     0.787093     44601 x 390
[2018-05-08 16:59:17,629] INFO:        3     0.778425     44602 x 390
[2018-05-08 16:59:17,834] INFO: AUC
[2018-05-08 16:59:17,834] INFO:      cv_mean: 0.778945
[2018-05-08 16:59:17,834] INFO:      cv_test: 0.785901
[2018-05-08 16:59:17,835] INFO: Time
[2018-05-08 16:59:17,835] INFO:      1 mins
[2018-05-08 16:59:17,835] INFO: --------------------------------------------------
[2018-05-08 17:00:23,864] INFO: tpe_transform took 0.002839 seconds
[2018-05-08 17:00:23,865] INFO: TPE using 4/4 trials with best loss -0.788916
[2018-05-08 17:00:23,867] INFO: ==================================================
[2018-05-08 17:00:23,867] INFO: Task
[2018-05-08 17:00:23,867] INFO:      Learner@clf_skl_rf_Id@23
[2018-05-08 17:00:23,868] INFO: Param
[2018-05-08 17:00:23,868] INFO:      max_depth: 8
[2018-05-08 17:00:23,868] INFO:      max_features: 0.7000000000000001
[2018-05-08 17:00:23,868] INFO:      min_samples_leaf: 9
[2018-05-08 17:00:23,868] INFO:      min_samples_split: 1
[2018-05-08 17:00:23,868] INFO:      n_estimators: 720
[2018-05-08 17:00:23,869] INFO:      n_jobs: 8
[2018-05-08 17:00:23,869] INFO:      random_state: 42
[2018-05-08 17:00:23,869] INFO:      verbose: 0
[2018-05-08 17:00:23,869] INFO: Result
[2018-05-08 17:00:23,869] INFO:      Run     AUC     Shape
[2018-05-08 17:00:25,776] INFO: job exception: JoblibValueError
___________________________________________________________________________
Multiprocessing exception:
...........................................................................
/Applications/PyCharm.app/Contents/helpers/pydev/pydev_run_in_console.py in <module>()
    145     except:
    146         sys.stderr.write("Console server didn't start\n")
    147         sys.stderr.flush()
    148         sys.exit(1)
    149 
--> 150     globals = run_file(file, None, None, is_module)
    151 
    152     interpreter.get_namespace().update(globals)
    153 
    154     interpreter.ShowConsole()

...........................................................................
/Applications/PyCharm.app/Contents/helpers/pydev/pydev_run_in_console.py in run_file(file='/Users/finup/Documents/github/kaggle/src/main.py', globals={'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'TaskOptimizer': <class 'task.TaskOptimizer'>, 'X_test':        02a300  0e0000  0ee000  0ei000  0eo000  0...  0       0       0  

[16725 rows x 390 columns], 'X_train':        02a300  0e0000  0ee000  0ei000  0eo000  0...  0       0       0  

[66902 rows x 390 columns], '__builtins__': <module 'builtins' (built-in)>, '__doc__': None, '__file__': '/Users/finup/Documents/github/kaggle/src/main.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': None, ...}, locals={'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'TaskOptimizer': <class 'task.TaskOptimizer'>, 'X_test':        02a300  0e0000  0ee000  0ei000  0eo000  0...  0       0       0  

[16725 rows x 390 columns], 'X_train':        02a300  0e0000  0ee000  0ei000  0eo000  0...  0       0       0  

[66902 rows x 390 columns], '__builtins__': <module 'builtins' (built-in)>, '__doc__': None, '__file__': '/Users/finup/Documents/github/kaggle/src/main.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': None, ...}, is_module=False)
     47         sys.path.insert(0, os.path.split(file)[0])
     48 
     49     print('Running %s' % file)
     50     try:
     51         if not is_module:
---> 52             pydev_imports.execfile(file, globals, locals)  # execute the script
        file = '/Users/finup/Documents/github/kaggle/src/main.py'
        globals = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'TaskOptimizer': <class 'task.TaskOptimizer'>, 'X_test':        02a300  0e0000  0ee000  0ei000  0eo000  0...  0       0       0  

[16725 rows x 390 columns], 'X_train':        02a300  0e0000  0ee000  0ei000  0eo000  0...  0       0       0  

[66902 rows x 390 columns], '__builtins__': <module 'builtins' (built-in)>, '__doc__': None, '__file__': '/Users/finup/Documents/github/kaggle/src/main.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': None, ...}
        locals = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'TaskOptimizer': <class 'task.TaskOptimizer'>, 'X_test':        02a300  0e0000  0ee000  0ei000  0eo000  0...  0       0       0  

[16725 rows x 390 columns], 'X_train':        02a300  0e0000  0ee000  0ei000  0eo000  0...  0       0       0  

[66902 rows x 390 columns], '__builtins__': <module 'builtins' (built-in)>, '__doc__': None, '__file__': '/Users/finup/Documents/github/kaggle/src/main.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': None, ...}
     53         else:
     54             # treat ':' as a seperator between module and entry point function
     55             # if there is no entry point we run we same as with -m switch. Otherwise we perform
     56             # an import and execute the entry point

...........................................................................
/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py in execfile(file='/Users/finup/Documents/github/kaggle/src/main.py', glob={'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'TaskOptimizer': <class 'task.TaskOptimizer'>, 'X_test':        02a300  0e0000  0ee000  0ei000  0eo000  0...  0       0       0  

[16725 rows x 390 columns], 'X_train':        02a300  0e0000  0ee000  0ei000  0eo000  0...  0       0       0  

[66902 rows x 390 columns], '__builtins__': <module 'builtins' (built-in)>, '__doc__': None, '__file__': '/Users/finup/Documents/github/kaggle/src/main.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': None, ...}, loc={'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'TaskOptimizer': <class 'task.TaskOptimizer'>, 'X_test':        02a300  0e0000  0ee000  0ei000  0eo000  0...  0       0       0  

[16725 rows x 390 columns], 'X_train':        02a300  0e0000  0ee000  0ei000  0eo000  0...  0       0       0  

[66902 rows x 390 columns], '__builtins__': <module 'builtins' (built-in)>, '__doc__': None, '__file__': '/Users/finup/Documents/github/kaggle/src/main.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': None, ...})
     13         contents = stream.read()
     14     finally:
     15         stream.close()
     16 
     17     #execute the script (note: it's important to compile first to have the filename set in debug mode)
---> 18     exec(compile(contents+"\n", file, 'exec'), glob, loc)
        contents = 'import pandas as pd\nfrom sklearn.feature_extract..., max_evals = 9, verbose=True)\noptimizer.run()\n\n\n'
        file = '/Users/finup/Documents/github/kaggle/src/main.py'
        glob = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'TaskOptimizer': <class 'task.TaskOptimizer'>, 'X_test':        02a300  0e0000  0ee000  0ei000  0eo000  0...  0       0       0  

[16725 rows x 390 columns], 'X_train':        02a300  0e0000  0ee000  0ei000  0eo000  0...  0       0       0  

[66902 rows x 390 columns], '__builtins__': <module 'builtins' (built-in)>, '__doc__': None, '__file__': '/Users/finup/Documents/github/kaggle/src/main.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': None, ...}
        loc = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'TaskOptimizer': <class 'task.TaskOptimizer'>, 'X_test':        02a300  0e0000  0ee000  0ei000  0eo000  0...  0       0       0  

[16725 rows x 390 columns], 'X_train':        02a300  0e0000  0ee000  0ei000  0eo000  0...  0       0       0  

[66902 rows x 390 columns], '__builtins__': <module 'builtins' (built-in)>, '__doc__': None, '__file__': '/Users/finup/Documents/github/kaggle/src/main.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': None, ...}

...........................................................................
/Users/finup/Documents/github/kaggle/src/main.py in <module>()
     16 y_test = test.is_apply
     17 
     18 
     19 #跑任务
     20 optimizer = TaskOptimizer(X_train, y_train, X_test, y_test, cv = 3, max_evals = 9, verbose=True)
---> 21 optimizer.run()
     22 
     23 

...........................................................................
/Users/finup/Documents/github/kaggle/src/task.py in run(self=<task.TaskOptimizer object>)
    220                 start = time.time()
    221                 trials = Trials()
    222                 logname = "%s_%s_%s.log" % (task_mode, learner, datetime.datetime.now().strftime("%Y-%m-%d-%H-%M"))
    223                 self.logger = logging_utils._get_logger(config.LOG_DIR, logname)
    224                 best = fmin(lambda param: self._obj(param, task_mode), self.param_space._build_space(learner),
--> 225                             tpe.suggest, self.max_evals, trials)
        self.max_evals = 9
        trials = <hyperopt.base.Trials object>
    226 
    227                 end = time.time()
    228                 time_cost = time_utils.time_diff(start, end)
    229                 self.logger.info("Hyperopt_Time")

...........................................................................
/Users/finup/anaconda3/envs/py3/lib/python3.6/site-packages/hyperopt/fmin.py in fmin(fn=<function TaskOptimizer.run.<locals>.<lambda>>, space={'max_depth': <hyperopt.pyll.base.Apply object>, 'max_features': <hyperopt.pyll.base.Apply object>, 'min_samples_leaf': <hyperopt.pyll.base.Apply object>, 'min_samples_split': <hyperopt.pyll.base.Apply object>, 'n_estimators': <hyperopt.pyll.base.Apply object>, 'n_jobs': 8, 'random_state': 42, 'verbose': 0}, algo=<function suggest>, max_evals=9, trials=<hyperopt.base.Trials object>, rstate=<mtrand.RandomState object>, allow_trials_fmin=True, pass_expr_memo_ctrl=None, catch_eval_exceptions=False, verbose=0, return_argmin=True)
    302             max_evals=max_evals,
    303             rstate=rstate,
    304             pass_expr_memo_ctrl=pass_expr_memo_ctrl,
    305             verbose=verbose,
    306             catch_eval_exceptions=catch_eval_exceptions,
--> 307             return_argmin=return_argmin,
        return_argmin = True
    308         )
    309 
    310     if trials is None:
    311         trials = base.Trials()

...........................................................................
/Users/finup/anaconda3/envs/py3/lib/python3.6/site-packages/hyperopt/base.py in fmin(self=<hyperopt.base.Trials object>, fn=<function TaskOptimizer.run.<locals>.<lambda>>, space={'max_depth': <hyperopt.pyll.base.Apply object>, 'max_features': <hyperopt.pyll.base.Apply object>, 'min_samples_leaf': <hyperopt.pyll.base.Apply object>, 'min_samples_split': <hyperopt.pyll.base.Apply object>, 'n_estimators': <hyperopt.pyll.base.Apply object>, 'n_jobs': 8, 'random_state': 42, 'verbose': 0}, algo=<function suggest>, max_evals=9, rstate=<mtrand.RandomState object>, verbose=0, pass_expr_memo_ctrl=None, catch_eval_exceptions=False, return_argmin=True)
    630             rstate=rstate,
    631             verbose=verbose,
    632             allow_trials_fmin=False,  # -- prevent recursion
    633             pass_expr_memo_ctrl=pass_expr_memo_ctrl,
    634             catch_eval_exceptions=catch_eval_exceptions,
--> 635             return_argmin=return_argmin)
        return_argmin = True
    636 
    637 
    638 def trials_from_docs(docs, validate=True, **kwargs):
    639     """Construct a Trials base class instance from a list of trials documents

...........................................................................
/Users/finup/anaconda3/envs/py3/lib/python3.6/site-packages/hyperopt/fmin.py in fmin(fn=<function TaskOptimizer.run.<locals>.<lambda>>, space={'max_depth': <hyperopt.pyll.base.Apply object>, 'max_features': <hyperopt.pyll.base.Apply object>, 'min_samples_leaf': <hyperopt.pyll.base.Apply object>, 'min_samples_split': <hyperopt.pyll.base.Apply object>, 'n_estimators': <hyperopt.pyll.base.Apply object>, 'n_jobs': 8, 'random_state': 42, 'verbose': 0}, algo=<function suggest>, max_evals=9, trials=<hyperopt.base.Trials object>, rstate=<mtrand.RandomState object>, allow_trials_fmin=False, pass_expr_memo_ctrl=None, catch_eval_exceptions=False, verbose=0, return_argmin=True)
    315 
    316     rval = FMinIter(algo, domain, trials, max_evals=max_evals,
    317                     rstate=rstate,
    318                     verbose=verbose)
    319     rval.catch_eval_exceptions = catch_eval_exceptions
--> 320     rval.exhaust()
        rval.exhaust = <bound method FMinIter.exhaust of <hyperopt.fmin.FMinIter object>>
    321     if return_argmin:
    322         return trials.argmin
    323 
    324 

...........................................................................
/Users/finup/anaconda3/envs/py3/lib/python3.6/site-packages/hyperopt/fmin.py in exhaust(self=<hyperopt.fmin.FMinIter object>)
    194             raise StopIteration()
    195         return self.trials
    196 
    197     def exhaust(self):
    198         n_done = len(self.trials)
--> 199         self.run(self.max_evals - n_done, block_until_done=self.async)
        self.run = <bound method FMinIter.run of <hyperopt.fmin.FMinIter object>>
        self.max_evals = 9
        n_done = 0
        self.async = False
    200         self.trials.refresh()
    201         return self
    202 
    203 

...........................................................................
/Users/finup/anaconda3/envs/py3/lib/python3.6/site-packages/hyperopt/fmin.py in run(self=<hyperopt.fmin.FMinIter object>, N=9, block_until_done=False)
    168             if self.async:
    169                 # -- wait for workers to fill in the trials
    170                 time.sleep(self.poll_interval_secs)
    171             else:
    172                 # -- loop over trials and do the jobs directly
--> 173                 self.serial_evaluate()
        self.serial_evaluate = <bound method FMinIter.serial_evaluate of <hyperopt.fmin.FMinIter object>>
    174 
    175             if stopped:
    176                 break
    177 

...........................................................................
/Users/finup/anaconda3/envs/py3/lib/python3.6/site-packages/hyperopt/fmin.py in serial_evaluate(self=<hyperopt.fmin.FMinIter object>, N=-1)
     87                 trial['book_time'] = now
     88                 trial['refresh_time'] = now
     89                 spec = base.spec_from_misc(trial['misc'])
     90                 ctrl = base.Ctrl(self.trials, current_trial=trial)
     91                 try:
---> 92                     result = self.domain.evaluate(spec, ctrl)
        result = undefined
        self.domain.evaluate = <bound method Domain.evaluate of <hyperopt.base.Domain object>>
        spec = {'max_depth': 8.0, 'max_features': 0.7000000000000001, 'min_samples_leaf': 9.0, 'min_samples_split': 1.0, 'n_estimators': 720.0}
        ctrl = <hyperopt.base.Ctrl object>
     93                 except Exception as e:
     94                     logger.info('job exception: %s' % str(e))
     95                     trial['state'] = base.JOB_STATE_ERROR
     96                     trial['misc']['error'] = (str(type(e)), str(e))

...........................................................................
/Users/finup/anaconda3/envs/py3/lib/python3.6/site-packages/hyperopt/base.py in evaluate(self=<hyperopt.base.Domain object>, config={'max_depth': 8.0, 'max_features': 0.7000000000000001, 'min_samples_leaf': 9.0, 'min_samples_split': 1.0, 'n_estimators': 720.0}, ctrl=<hyperopt.base.Ctrl object>, attach_attachments=True)
    835             #    or the normal Python part (self.fn)
    836             pyll_rval = pyll.rec_eval(
    837                 self.expr,
    838                 memo=memo,
    839                 print_node_on_error=self.rec_eval_print_node_on_error)
--> 840             rval = self.fn(pyll_rval)
        rval = undefined
        self.fn = <function TaskOptimizer.run.<locals>.<lambda>>
        pyll_rval = {'max_depth': 8, 'max_features': 0.7000000000000001, 'min_samples_leaf': 9, 'min_samples_split': 1, 'n_estimators': 720, 'n_jobs': 8, 'random_state': 42, 'verbose': 0}
    841 
    842         if isinstance(rval, (float, int, np.number)):
    843             dict_rval = {'loss': float(rval), 'status': STATUS_OK}
    844         else:

...........................................................................
/Users/finup/Documents/github/kaggle/src/task.py in <lambda>(param={'max_depth': 8, 'max_features': 0.7000000000000001, 'min_samples_leaf': 9, 'min_samples_split': 1, 'n_estimators': 720, 'n_jobs': 8, 'random_state': 42, 'verbose': 0})
    219                 self.leaner_name = learner
    220                 start = time.time()
    221                 trials = Trials()
    222                 logname = "%s_%s_%s.log" % (task_mode, learner, datetime.datetime.now().strftime("%Y-%m-%d-%H-%M"))
    223                 self.logger = logging_utils._get_logger(config.LOG_DIR, logname)
--> 224                 best = fmin(lambda param: self._obj(param, task_mode), self.param_space._build_space(learner),
        param = {'max_depth': 8, 'max_features': 0.7000000000000001, 'min_samples_leaf': 9, 'min_samples_split': 1, 'n_estimators': 720, 'n_jobs': 8, 'random_state': 42, 'verbose': 0}
    225                             tpe.suggest, self.max_evals, trials)
    226 
    227                 end = time.time()
    228                 time_cost = time_utils.time_diff(start, end)

...........................................................................
/Users/finup/Documents/github/kaggle/src/task.py in _obj(self=<task.TaskOptimizer object>, param_dict={'max_depth': 8, 'max_features': 0.7000000000000001, 'min_samples_leaf': 9, 'min_samples_split': 1, 'n_estimators': 720, 'n_jobs': 8, 'random_state': 42, 'verbose': 0}, task_mode='single')
    195             stacking_level1_train = pd.concat([pd.read_csv(f) for f in train_fnames], axis=1)
    196             stacking_level1_test = pd.concat([pd.read_csv(f) for f in test_fnames], axis = 1)
    197             self.task = Task(learner, stacking_level1_train, self.y_train, stacking_level1_test, self.y_test,
    198                              self.n_iter,
    199                              suffix, self.logger, self.verbose)
--> 200         self.task.go()
        self.task.go = <bound method Task.go of <task.Task object>>
    201         result = {
    202             "loss": -self.task.test_auc,
    203             "attachments": {
    204                 "train_auc": self.task.train_auc,

...........................................................................
/Users/finup/Documents/github/kaggle/src/task.py in go(self=<task.Task object>)
    160         pd.DataFrame({"prediction": y_pred_test}).to_csv(fname, index=False)
    161 
    162         return self
    163 
    164     def go(self):
--> 165         self.cv()
        self.cv = <bound method Task.cv of <task.Task object>>
    166         self.refit()
    167         return self
    168 
    169 

...........................................................................
/Users/finup/Documents/github/kaggle/src/task.py in cv(self=<task.Task object>)
    106         i = 0
    107         for train_index, valid_index in shuffle.split(self.X_train, self.y_train):
    108             i += 1
    109             X_train_cv, y_train_cv, X_valid_cv, y_valid_cv = self.X_train.iloc[train_index, :], self.y_train[
    110                 train_index], self.X_train.iloc[valid_index, :], self.y_train[valid_index]
--> 111             self.learner.fit(X_train_cv, y_train_cv)
        self.learner.fit = <bound method Learner.fit of <task.Learner object>>
        X_train_cv =        02a300  0e0000  0ee000  0ei000  0eo000  0...  0       0       0  

[44601 rows x 390 columns]
        y_train_cv = 22301    0
22302    0
22303    0
22304    0
2230...    0
Name: is_apply, Length: 44601, dtype: int64
    112             y_pred = self.learner.predict_proba(X_valid_cv)
    113             y_pred_test = self.learner.predict_proba(self.X_test)
    114             auc_cv[i - 1] = roc_auc_score(y_valid_cv, y_pred)
    115             stacking_feature_train[valid_index] = y_pred

...........................................................................
/Users/finup/Documents/github/kaggle/src/task.py in fit(self=<task.Learner object>, X=       02a300  0e0000  0ee000  0ei000  0eo000  0...  0       0       0  

[44601 rows x 390 columns], y=22301    0
22302    0
22303    0
22304    0
2230...    0
Name: is_apply, Length: 44601, dtype: int64)
     50 
     51     def __str__(self):
     52         return self.learner_name
     53 
     54     def fit(self, X, y):
---> 55         return self.learner.fit(X, y)
        self.learner.fit = <bound method BaseForest.fit of RandomForestClas...ate=42,
            verbose=0, warm_start=False)>
        X =        02a300  0e0000  0ee000  0ei000  0eo000  0...  0       0       0  

[44601 rows x 390 columns]
        y = 22301    0
22302    0
22303    0
22304    0
2230...    0
Name: is_apply, Length: 44601, dtype: int64
     56 
     57     def predict(self, X):
     58         return self.learner.predict(X)
     59 

...........................................................................
/Users/finup/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/ensemble/forest.py in fit(self=RandomForestClassifier(bootstrap=True, class_wei...tate=42,
            verbose=0, warm_start=False), X=array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],
    ....,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), y=array([[ 0.],
       [ 0.],
       [ 0.],
       ..., 
       [ 0.],
       [ 0.],
       [ 0.]]), sample_weight=None)
    323             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
    324                              backend="threading")(
    325                 delayed(_parallel_build_trees)(
    326                     t, self, X, y, sample_weight, i, len(trees),
    327                     verbose=self.verbose, class_weight=self.class_weight)
--> 328                 for i, t in enumerate(trees))
        i = 719
    329 
    330             # Collect newly grown trees
    331             self.estimators_.extend(trees)
    332 

...........................................................................
/Users/finup/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=8), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)
    784             if pre_dispatch == "all" or n_jobs == 1:
    785                 # The iterable was consumed all at once by the above for loop.
    786                 # No need to wait for async callbacks to trigger to
    787                 # consumption.
    788                 self._iterating = False
--> 789             self.retrieve()
        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=8)>
    790             # Make sure that we get a last message telling us we are done
    791             elapsed_time = time.time() - self._start_time
    792             self._print('Done %3i out of %3i | elapsed: %s finished',
    793                         (len(self._output), len(self._output),

---------------------------------------------------------------------------
Sub-process traceback:
---------------------------------------------------------------------------
ValueError                                         Tue May  8 17:00:24 2018
PID: 83735         Python 3.6.4: /Users/finup/anaconda3/envs/py3/bin/python
...........................................................................
/Users/finup/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _parallel_build_trees>, (DecisionTreeClassifier(class_weight=None, criter...        random_state=1608637542, splitter='best'), RandomForestClassifier(bootstrap=True, class_wei...tate=42,
            verbose=0, warm_start=False), array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],
    ....,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), array([[ 0.],
       [ 0.],
       [ 0.],
       ..., 
       [ 0.],
       [ 0.],
       [ 0.]]), None, 0, 720), {'class_weight': None, 'verbose': 0})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/Users/finup/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _parallel_build_trees>
        args = (DecisionTreeClassifier(class_weight=None, criter...        random_state=1608637542, splitter='best'), RandomForestClassifier(bootstrap=True, class_wei...tate=42,
            verbose=0, warm_start=False), array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],
    ....,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), array([[ 0.],
       [ 0.],
       [ 0.],
       ..., 
       [ 0.],
       [ 0.],
       [ 0.]]), None, 0, 720)
        kwargs = {'class_weight': None, 'verbose': 0}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/Users/finup/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/ensemble/forest.py in _parallel_build_trees(tree=DecisionTreeClassifier(class_weight=None, criter...        random_state=1608637542, splitter='best'), forest=RandomForestClassifier(bootstrap=True, class_wei...tate=42,
            verbose=0, warm_start=False), X=array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],
    ....,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), y=array([[ 0.],
       [ 0.],
       [ 0.],
       ..., 
       [ 0.],
       [ 0.],
       [ 0.]]), sample_weight=None, tree_idx=0, n_trees=720, verbose=0, class_weight=None)
    116                 warnings.simplefilter('ignore', DeprecationWarning)
    117                 curr_sample_weight *= compute_sample_weight('auto', y, indices)
    118         elif class_weight == 'balanced_subsample':
    119             curr_sample_weight *= compute_sample_weight('balanced', y, indices)
    120 
--> 121         tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)
        tree.fit = <bound method DecisionTreeClassifier.fit of Deci...       random_state=1608637542, splitter='best')>
        X = array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],
    ....,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)
        y = array([[ 0.],
       [ 0.],
       [ 0.],
       ..., 
       [ 0.],
       [ 0.],
       [ 0.]])
        sample_weight = None
        curr_sample_weight = array([ 0.,  2.,  3., ...,  1.,  2.,  2.])
    122     else:
    123         tree.fit(X, y, sample_weight=sample_weight, check_input=False)
    124 
    125     return tree

...........................................................................
/Users/finup/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/tree/tree.py in fit(self=DecisionTreeClassifier(class_weight=None, criter...        random_state=1608637542, splitter='best'), X=array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],
    ....,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), y=array([[ 0.],
       [ 0.],
       [ 0.],
       ..., 
       [ 0.],
       [ 0.],
       [ 0.]]), sample_weight=array([ 0.,  2.,  3., ...,  1.,  2.,  2.]), check_input=False, X_idx_sorted=None)
    785 
    786         super(DecisionTreeClassifier, self).fit(
    787             X, y,
    788             sample_weight=sample_weight,
    789             check_input=check_input,
--> 790             X_idx_sorted=X_idx_sorted)
        X_idx_sorted = None
    791         return self
    792 
    793     def predict_proba(self, X, check_input=True):
    794         """Predict class probabilities of the input samples X.

...........................................................................
/Users/finup/anaconda3/envs/py3/lib/python3.6/site-packages/sklearn/tree/tree.py in fit(self=DecisionTreeClassifier(class_weight=None, criter...        random_state=1608637542, splitter='best'), X=array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],
    ....,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), y=array([[ 0.],
       [ 0.],
       [ 0.],
       ..., 
       [ 0.],
       [ 0.],
       [ 0.]]), sample_weight=array([ 0.,  2.,  3., ...,  1.,  2.,  2.]), check_input=False, X_idx_sorted=None)
    189         if isinstance(self.min_samples_split, (numbers.Integral, np.integer)):
    190             if not 2 <= self.min_samples_split:
    191                 raise ValueError("min_samples_split must be an integer "
    192                                  "greater than 1 or a float in (0.0, 1.0]; "
    193                                  "got the integer %s"
--> 194                                  % self.min_samples_split)
        self.min_samples_split = 1
    195             min_samples_split = self.min_samples_split
    196         else:  # float
    197             if not 0. < self.min_samples_split <= 1.:
    198                 raise ValueError("min_samples_split must be an integer "

ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1
___________________________________________________________________________
